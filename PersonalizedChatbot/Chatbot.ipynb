{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing The Neccesary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import json\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Query-Based Intents for the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents = {\n",
    "  \"intents\": [\n",
    "    {\n",
    "      \"tag\": \"greeting\",\n",
    "      \"patterns\": [\"Hi\", \"Hello\", \"Hey\", \"Good day\", \"How are you?\"],\n",
    "      \"responses\": [\"Hello!\", \"Good to see you!\", \"Hi there, how can I help?\"]\n",
    "    },\n",
    "    {\n",
    "      \"tag\": \"farewell\",\n",
    "      \"patterns\": [\"Goodbye\", \"Bye\", \"See you later\", \"Talk to you later\"],\n",
    "      \"responses\": [\"Sad to see you go :(\", \"Goodbye!\", \"Come back soon!\"]\n",
    "\n",
    "    },\n",
    "    {\n",
    "      \"tag\": \"creator\",\n",
    "      \"patterns\": [\"Who created you?\", \"Who is your developer?\", \"Who made you?\"],\n",
    "      \"responses\": [\"I was created by Shivani & Arthiga\"]\n",
    "\n",
    "    },\n",
    "    {\n",
    "      \"tag\": \"identity\",\n",
    "      \"patterns\": [\"What is your name?\", \"What should I call you?\", \"Who are you?\",\"What are you\",\"Introduce Yourself\"],\n",
    "      \"responses\": [\"You can call me Jarvis. I'm a Chatbot.\"]\n",
    "\n",
    "    },\n",
    "    \n",
    "    {\n",
    "      \"tag\": \"casual_greeting\",\n",
    "      \"patterns\": [\"What's up?\", \"How are you?\", \"How you doing?\"],\n",
    "       \"responses\": [\"I'm here to assist you with any questions or information you need. How can I assist you today?\"]\n",
    "\n",
    "     },\n",
    "    {\n",
    "      \"tag\": \"good_morning\",\n",
    "      \"patterns\": [\"Good morning\", \"Morning\"],\n",
    "      \"responses\": [\"Good morning! How can I assist you today?\"]\n",
    "\n",
    "     },\n",
    "     {\n",
    "       \"tag\": \"good_afternoon\",\n",
    "       \"patterns\": [\"Good afternoon\", \"Afternoon\"],\n",
    "        \"responses\": [\"Good afternoon! How can I assist you today?\"]\n",
    "\n",
    "      },\n",
    "      {\n",
    "      \"tag\": \"good_evening\",\n",
    "      \"patterns\": [\"Good evening\", \"Evening\"],\n",
    "       \"responses\": [\"Good evening! How can I assist you today?\"]\n",
    "\n",
    "         },\n",
    "          {\n",
    "        \"tag\": \"thank_you\",\n",
    "        \"patterns\": [\"Thank you\", \"Thanks\"],\n",
    "        \"responses\": [\"You're welcome! If you have any more questions, feel free to ask.\"]\n",
    "\n",
    "        },\n",
    "       {\n",
    "       \"tag\": \"sorry\",\n",
    "      \"patterns\": [\"Sorry\", \"Apologies\"],\n",
    "       \"responses\": [\"No problem! If there's anything else you need assistance with, feel free to let me know.\"]\n",
    "\n",
    "    },\n",
    "    {\n",
    "         \"tag\": \"Total_Failures\",\n",
    "      \"patterns\": [\"Total Failures\",\"Today count of machine failure?\",\"How many machines failed?\"],\n",
    "       \"responses\": [\"Here are the results!\"]\n",
    "    },\n",
    "    {\n",
    "        \"tag\": \"Total_Non_Failures\",\n",
    "      \"patterns\": [\"Total Avalible machines\",\"Today count of machine present?\",\"How many machines available?\"],\n",
    "       \"responses\": [\"Here are the results!\"]\n",
    "    }  ,\n",
    "    { \"tag\": \"Common_Failure_Conditions\",\n",
    "      \"patterns\": [\"what are the conditions for failure\",\"common failure conditions?\"],\n",
    "       \"responses\": [\"Here are the results!\"]\n",
    "        \n",
    "    },\n",
    "    {\n",
    "        \"tag\": \"Average_Temperature\",\n",
    "      \"patterns\": [\"what is the average temperature for failure\",\"Average Temperature for failure\"],\n",
    "       \"responses\": [\"Here are the results!\"]\n",
    "    },\n",
    "    {\n",
    "        \"tag\": \"High_VOC\",\n",
    "      \"patterns\": [\"what is the Failure Rate with High VOC \"],\n",
    "       \"responses\": [\"Here are the results!\"]\n",
    "    },\n",
    "    {\n",
    "        \"tag\": \"High_Footfall\",\n",
    "      \"patterns\": [\"what is the Failure Rate with High Footfall? \"],\n",
    "       \"responses\": [\"Here are the results!\"]\n",
    "    },\n",
    "  {\n",
    "       \"tag\": \"CS_Level\",\n",
    "      \"patterns\": [\"what is the Failure Rate with CS Level? \"],\n",
    "       \"responses\": [\"Here are the results!\"]\n",
    "  },\n",
    "  {\n",
    "       \"tag\": \"Poor_Air_Quality \",\n",
    "      \"patterns\": [\"what is the Failure Rate with Poor Air Quality ? \"],\n",
    "       \"responses\": [\"Here are the results!\"]\n",
    "  },\n",
    "  {\n",
    "       \"tag\": \"High_IP\",\n",
    "      \"patterns\": [\"what is the Failure Rate with High IP? \"],\n",
    "       \"responses\": [\"Here are the results!\"]\n",
    "  }\n",
    "]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading NLP Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rockstar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rockstar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Data and Synonym Augmentation for Intent Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 documents\n",
      "19 classes ['Average_Temperature', 'CS_Level', 'Common_Failure_Conditions', 'High_Footfall', 'High_IP', 'High_VOC', 'Poor_Air_Quality ', 'Total_Failures', 'Total_Non_Failures', 'casual_greeting', 'creator', 'farewell', 'good_afternoon', 'good_evening', 'good_morning', 'greeting', 'identity', 'sorry', 'thank_you']\n",
      "65 unique lemmatized words [\"'s\", 'afternoon', 'air', 'apology', 'are', 'available', 'avalible', 'average', 'bye', 'c', 'call', 'common', 'condition', 'count', 'created', 'day', 'developer', 'doing', 'evening', 'failed', 'failure', 'footfall', 'for', 'good', 'goodbye', 'hello', 'hey', 'hi', 'high', 'how', 'i', 'introduce', 'ip', 'is', 'later', 'level', 'machine', 'made', 'many', 'morning', 'name', 'of', 'poor', 'present', 'quality', 'rate', 'see', 'should', 'sorry', 'talk', 'temperature', 'thank', 'thanks', 'the', 'to', 'today', 'total', 'up', 'voc', 'what', 'who', 'with', 'you', 'your', 'yourself']\n"
     ]
    }
   ],
   "source": [
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?']\n",
    "\n",
    "\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        # Tokenize each word in the sentence\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        # Add to our words list\n",
    "        words.extend(w)\n",
    "        # Add to documents in our corpus\n",
    "        documents.append((w, intent['tag']))\n",
    "        # Add to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "\n",
    "words = [stemmer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "print(len(documents), \"documents\")\n",
    "print(len(classes), \"classes\", classes)\n",
    "print(len(words), \"unique lemmatized words\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training data: 45\n",
      "Length of augmented data: 2153\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Let's assume 'training' and 'augmented_data' are lists of [bag, output_row] pairs\n",
    "training = []\n",
    "output = []\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "# Training set, bag of words for each sentence\n",
    "for doc in documents:\n",
    "    # Initialize our bag of words\n",
    "    bag = []\n",
    "    # List of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # Lemmatize each word\n",
    "    pattern_words = [stemmer.lemmatize(word.lower()) for word in pattern_words]\n",
    "    # Create our bag of words array\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "    # Output is a '0' for each tag and '1' for the current tag\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "\n",
    "    training.append([bag, output_row])\n",
    "\n",
    "random.shuffle(training)\n",
    "\n",
    "def synonym_replacement(tokens, limit):\n",
    "    augmented_sentences = []\n",
    "    for i in range(len(tokens)):\n",
    "        synonyms = []\n",
    "        for syn in wordnet.synsets(tokens[i]):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonyms.append(lemma.name())\n",
    "        if len(synonyms) > 0:\n",
    "            num_augmentations = min(limit, len(synonyms))\n",
    "            sampled_synonyms = random.sample(synonyms, num_augmentations)\n",
    "            for synonym in sampled_synonyms:\n",
    "                augmented_tokens = tokens[:i] + [synonym] + tokens[i + 1:]\n",
    "                augmented_sentences.append(' '.join(augmented_tokens))\n",
    "    return augmented_sentences\n",
    "\n",
    "# Augment the training data using synonym replacement\n",
    "augmented_data = []\n",
    "limit_per_tag = 100\n",
    "\n",
    "for i, doc in enumerate(training):\n",
    "    bag, output_row = doc\n",
    "    tokens = [words[j] for j in range(len(words)) if bag[j] == 1]\n",
    "    augmented_sentences = synonym_replacement(tokens, limit_per_tag)\n",
    "    for augmented_sentence in augmented_sentences:\n",
    "        augmented_bag = [1 if word in augmented_sentence.split() else 0 for word in words]\n",
    "        augmented_data.append([augmented_bag, output_row])\n",
    "\n",
    "# Check the structure of the data and ensure consistency\n",
    "print(\"Length of training data:\", len(training))\n",
    "print(\"Length of augmented data:\", len(augmented_data))\n",
    "\n",
    "# Combine training and augmented data (no need for numpy arrays if the structure is inconsistent)\n",
    "combined_data = training + augmented_data\n",
    "random.shuffle(combined_data)\n",
    "\n",
    "# Optional: Convert to numpy array if shapes are consistent, but this should work as is\n",
    "# combined_data = np.array(combined_data, dtype=object)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def separate_data_by_tags(data):\n",
    "    data_by_tags = {}\n",
    "    for d in data:\n",
    "        tag = tuple(d[1])\n",
    "        if tag not in data_by_tags:\n",
    "            data_by_tags[tag] = []\n",
    "        data_by_tags[tag].append(d)\n",
    "    return data_by_tags.values()\n",
    "\n",
    "\n",
    "separated_data = separate_data_by_tags(combined_data)\n",
    "\n",
    "# Lists to store training and testing data\n",
    "training_data = []\n",
    "testing_data = []\n",
    "\n",
    "# Split each tag's data into training and testing sets\n",
    "for tag_data in separated_data:\n",
    "    train_data, test_data = train_test_split(tag_data, test_size=0.2, random_state=42)\n",
    "    training_data.extend(train_data)\n",
    "    testing_data.extend(test_data)\n",
    "\n",
    "\n",
    "random.shuffle(training_data)\n",
    "random.shuffle(testing_data)\n",
    "\n",
    "# Convert training and testing data back to np.array\n",
    "train_x = np.array([d[0] for d in training_data])\n",
    "train_y = np.array([d[1] for d in training_data])\n",
    "test_x = np.array([d[0] for d in testing_data])\n",
    "test_y = np.array([d[1] for d in testing_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        output = self.softmax(x)\n",
    "        return output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "def accuracy(predictions, targets):\n",
    "    predicted_labels = torch.argmax(predictions, dim=1)\n",
    "    true_labels = torch.argmax(targets, dim=1)\n",
    "    correct = (predicted_labels == true_labels).sum().item()\n",
    "    total = targets.size(0)\n",
    "    return correct / total\n",
    "\n",
    "def test_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    num_batches = len(test_loader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            total_accuracy += accuracy(outputs, targets) * inputs.size(0)\n",
    "\n",
    "    average_loss = total_loss / len(test_loader.dataset)\n",
    "    average_accuracy = total_accuracy / len(test_loader.dataset)\n",
    "    return average_loss, average_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Training Loss: 0.2054, Training Accuracy: 0.0828\n",
      "Epoch [1/50], Testing Loss: 0.2040, Testing Accuracy: 0.1637\n",
      "Epoch [2/50], Training Loss: 0.2027, Training Accuracy: 0.1621\n",
      "Epoch [2/50], Testing Loss: 0.2008, Testing Accuracy: 0.1480\n",
      "Epoch [3/50], Training Loss: 0.1990, Training Accuracy: 0.1513\n",
      "Epoch [3/50], Testing Loss: 0.1962, Testing Accuracy: 0.1682\n",
      "Epoch [4/50], Training Loss: 0.1937, Training Accuracy: 0.1781\n",
      "Epoch [4/50], Testing Loss: 0.1897, Testing Accuracy: 0.2063\n",
      "Epoch [5/50], Training Loss: 0.1864, Training Accuracy: 0.1986\n",
      "Epoch [5/50], Testing Loss: 0.1814, Testing Accuracy: 0.2085\n",
      "Epoch [6/50], Training Loss: 0.1775, Training Accuracy: 0.2049\n",
      "Epoch [6/50], Testing Loss: 0.1717, Testing Accuracy: 0.2578\n",
      "Epoch [7/50], Training Loss: 0.1677, Training Accuracy: 0.3282\n",
      "Epoch [7/50], Testing Loss: 0.1620, Testing Accuracy: 0.3677\n",
      "Epoch [8/50], Training Loss: 0.1580, Training Accuracy: 0.3756\n",
      "Epoch [8/50], Testing Loss: 0.1527, Testing Accuracy: 0.3722\n",
      "Epoch [9/50], Training Loss: 0.1487, Training Accuracy: 0.3990\n",
      "Epoch [9/50], Testing Loss: 0.1435, Testing Accuracy: 0.3901\n",
      "Epoch [10/50], Training Loss: 0.1397, Training Accuracy: 0.4075\n",
      "Epoch [10/50], Testing Loss: 0.1347, Testing Accuracy: 0.4103\n",
      "Epoch [11/50], Training Loss: 0.1312, Training Accuracy: 0.4589\n",
      "Epoch [11/50], Testing Loss: 0.1265, Testing Accuracy: 0.4933\n",
      "Epoch [12/50], Training Loss: 0.1232, Training Accuracy: 0.5131\n",
      "Epoch [12/50], Testing Loss: 0.1190, Testing Accuracy: 0.5471\n",
      "Epoch [13/50], Training Loss: 0.1160, Training Accuracy: 0.5439\n",
      "Epoch [13/50], Testing Loss: 0.1122, Testing Accuracy: 0.5583\n",
      "Epoch [14/50], Training Loss: 0.1093, Training Accuracy: 0.5628\n",
      "Epoch [14/50], Testing Loss: 0.1059, Testing Accuracy: 0.5852\n",
      "Epoch [15/50], Training Loss: 0.1031, Training Accuracy: 0.6073\n",
      "Epoch [15/50], Testing Loss: 0.1001, Testing Accuracy: 0.6457\n",
      "Epoch [16/50], Training Loss: 0.0973, Training Accuracy: 0.6404\n",
      "Epoch [16/50], Testing Loss: 0.0945, Testing Accuracy: 0.6637\n",
      "Epoch [17/50], Training Loss: 0.0917, Training Accuracy: 0.6718\n",
      "Epoch [17/50], Testing Loss: 0.0891, Testing Accuracy: 0.6704\n",
      "Epoch [18/50], Training Loss: 0.0862, Training Accuracy: 0.6804\n",
      "Epoch [18/50], Testing Loss: 0.0838, Testing Accuracy: 0.6996\n",
      "Epoch [19/50], Training Loss: 0.0807, Training Accuracy: 0.7255\n",
      "Epoch [19/50], Testing Loss: 0.0785, Testing Accuracy: 0.7287\n",
      "Epoch [20/50], Training Loss: 0.0753, Training Accuracy: 0.7529\n",
      "Epoch [20/50], Testing Loss: 0.0733, Testing Accuracy: 0.7892\n",
      "Epoch [21/50], Training Loss: 0.0701, Training Accuracy: 0.7928\n",
      "Epoch [21/50], Testing Loss: 0.0682, Testing Accuracy: 0.8027\n",
      "Epoch [22/50], Training Loss: 0.0650, Training Accuracy: 0.8076\n",
      "Epoch [22/50], Testing Loss: 0.0635, Testing Accuracy: 0.8229\n",
      "Epoch [23/50], Training Loss: 0.0602, Training Accuracy: 0.8345\n",
      "Epoch [23/50], Testing Loss: 0.0590, Testing Accuracy: 0.8543\n",
      "Epoch [24/50], Training Loss: 0.0557, Training Accuracy: 0.8761\n",
      "Epoch [24/50], Testing Loss: 0.0549, Testing Accuracy: 0.8610\n",
      "Epoch [25/50], Training Loss: 0.0516, Training Accuracy: 0.8767\n",
      "Epoch [25/50], Testing Loss: 0.0512, Testing Accuracy: 0.8565\n",
      "Epoch [26/50], Training Loss: 0.0480, Training Accuracy: 0.8727\n",
      "Epoch [26/50], Testing Loss: 0.0480, Testing Accuracy: 0.8498\n",
      "Epoch [27/50], Training Loss: 0.0448, Training Accuracy: 0.8756\n",
      "Epoch [27/50], Testing Loss: 0.0451, Testing Accuracy: 0.8498\n",
      "Epoch [28/50], Training Loss: 0.0419, Training Accuracy: 0.8756\n",
      "Epoch [28/50], Testing Loss: 0.0427, Testing Accuracy: 0.8789\n",
      "Epoch [29/50], Training Loss: 0.0392, Training Accuracy: 0.9007\n",
      "Epoch [29/50], Testing Loss: 0.0404, Testing Accuracy: 0.8834\n",
      "Epoch [30/50], Training Loss: 0.0368, Training Accuracy: 0.9024\n",
      "Epoch [30/50], Testing Loss: 0.0383, Testing Accuracy: 0.8901\n",
      "Epoch [31/50], Training Loss: 0.0347, Training Accuracy: 0.9092\n",
      "Epoch [31/50], Testing Loss: 0.0365, Testing Accuracy: 0.8901\n",
      "Epoch [32/50], Training Loss: 0.0328, Training Accuracy: 0.9167\n",
      "Epoch [32/50], Testing Loss: 0.0349, Testing Accuracy: 0.8991\n",
      "Epoch [33/50], Training Loss: 0.0310, Training Accuracy: 0.9144\n",
      "Epoch [33/50], Testing Loss: 0.0334, Testing Accuracy: 0.8991\n",
      "Epoch [34/50], Training Loss: 0.0294, Training Accuracy: 0.9195\n",
      "Epoch [34/50], Testing Loss: 0.0321, Testing Accuracy: 0.8991\n",
      "Epoch [35/50], Training Loss: 0.0281, Training Accuracy: 0.9207\n",
      "Epoch [35/50], Testing Loss: 0.0310, Testing Accuracy: 0.8991\n",
      "Epoch [36/50], Training Loss: 0.0269, Training Accuracy: 0.9195\n",
      "Epoch [36/50], Testing Loss: 0.0299, Testing Accuracy: 0.8991\n",
      "Epoch [37/50], Training Loss: 0.0259, Training Accuracy: 0.9207\n",
      "Epoch [37/50], Testing Loss: 0.0290, Testing Accuracy: 0.8991\n",
      "Epoch [38/50], Training Loss: 0.0250, Training Accuracy: 0.9207\n",
      "Epoch [38/50], Testing Loss: 0.0282, Testing Accuracy: 0.9013\n",
      "Epoch [39/50], Training Loss: 0.0241, Training Accuracy: 0.9218\n",
      "Epoch [39/50], Testing Loss: 0.0276, Testing Accuracy: 0.9013\n",
      "Epoch [40/50], Training Loss: 0.0235, Training Accuracy: 0.9224\n",
      "Epoch [40/50], Testing Loss: 0.0269, Testing Accuracy: 0.9013\n",
      "Epoch [41/50], Training Loss: 0.0228, Training Accuracy: 0.9224\n",
      "Epoch [41/50], Testing Loss: 0.0264, Testing Accuracy: 0.9013\n",
      "Epoch [42/50], Training Loss: 0.0223, Training Accuracy: 0.9235\n",
      "Epoch [42/50], Testing Loss: 0.0260, Testing Accuracy: 0.9013\n",
      "Epoch [43/50], Training Loss: 0.0218, Training Accuracy: 0.9235\n",
      "Epoch [43/50], Testing Loss: 0.0256, Testing Accuracy: 0.9013\n",
      "Epoch [44/50], Training Loss: 0.0213, Training Accuracy: 0.9275\n",
      "Epoch [44/50], Testing Loss: 0.0252, Testing Accuracy: 0.9013\n",
      "Epoch [45/50], Training Loss: 0.0210, Training Accuracy: 0.9275\n",
      "Epoch [45/50], Testing Loss: 0.0248, Testing Accuracy: 0.9013\n",
      "Epoch [46/50], Training Loss: 0.0205, Training Accuracy: 0.9275\n",
      "Epoch [46/50], Testing Loss: 0.0247, Testing Accuracy: 0.9013\n",
      "Epoch [47/50], Training Loss: 0.0203, Training Accuracy: 0.9264\n",
      "Epoch [47/50], Testing Loss: 0.0243, Testing Accuracy: 0.9013\n",
      "Epoch [48/50], Training Loss: 0.0199, Training Accuracy: 0.9275\n",
      "Epoch [48/50], Testing Loss: 0.0241, Testing Accuracy: 0.9013\n",
      "Epoch [49/50], Training Loss: 0.0196, Training Accuracy: 0.9287\n",
      "Epoch [49/50], Testing Loss: 0.0238, Testing Accuracy: 0.9013\n",
      "Epoch [50/50], Training Loss: 0.0194, Training Accuracy: 0.9275\n",
      "Epoch [50/50], Testing Loss: 0.0235, Testing Accuracy: 0.9013\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader for training and testing data\n",
    "train_x = torch.tensor(train_x).float()\n",
    "train_y = torch.tensor(train_y).float()\n",
    "test_x = torch.tensor(test_x).float()\n",
    "test_y = torch.tensor(test_y).float()\n",
    "\n",
    "batch_size = 64\n",
    "train_dataset = CustomDataset(train_x, train_y)\n",
    "test_dataset = CustomDataset(test_x, test_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the model, loss function, and optimizer\n",
    "input_size = len(train_x[0])\n",
    "hidden_size = 8\n",
    "output_size = len(train_y[0])\n",
    "model = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Train the model and evaluate on the testing set\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_acc += accuracy(outputs, targets) * inputs.size(0)\n",
    "\n",
    "    # Calculate average training loss and accuracy for the epoch\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_acc = running_acc / len(train_loader.dataset)\n",
    "\n",
    "    # Print training loss and accuracy for each epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.4f}, Training Accuracy: {epoch_acc:.4f}\")\n",
    "\n",
    "    # Evaluate on the testing set\n",
    "    test_loss, test_accuracy = test_model(model, test_loader, criterion)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Testing Loss: {test_loss:.4f}, Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, input_size, hidden_size, output_size):\n",
    "    model = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Function to preprocess the input sentence\n",
    "def preprocess_sentence(sentence, words):\n",
    "    sentence_words = sentence.lower().split()\n",
    "    sentence_words = [word for word in sentence_words if word in words]\n",
    "    return sentence_words\n",
    "\n",
    "# Function to convert the preprocessed sentence into a feature vector\n",
    "def sentence_to_features(sentence_words, words):\n",
    "    features = [1 if word in sentence_words else 0 for word in words]\n",
    "    return torch.tensor(features).float().unsqueeze(0)\n",
    "\n",
    "# Function to generate a response using the trained model\n",
    "def generate_response(sentence, model, words, classes):\n",
    "    sentence_words = preprocess_sentence(sentence, words)\n",
    "    if len(sentence_words) == 0:\n",
    "        return \"I'm not quite sure I get it. Could you explain that a bit more or put it another way?\"\n",
    "\n",
    "    features = sentence_to_features(sentence_words, words)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(features)\n",
    "\n",
    "    probabilities, predicted_class = torch.max(outputs, dim=1)\n",
    "    confidence = probabilities.item()\n",
    "    predicted_tag = classes[predicted_class.item()]\n",
    "\n",
    "    if confidence > 0.5:\n",
    "        for intent in intents['intents']:\n",
    "            if intent['tag'] == predicted_tag:\n",
    "                return random.choice(intent['responses'])\n",
    "\n",
    "    return \"I'm a bit lost on how to respond. Could you explain a bit more?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
